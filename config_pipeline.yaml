models:
  segmentation_tflite: "trained_models/segmentation/exported/segmentation_model.tflite" # Check if this is the correct exported model path/name
  classification_tflite: "trained_models/classification/exported/classification_model.tflite" # Check if this is the correct exported model path/name
  classification_labels: "trained_models/classification/label_map.json" # Path to the label map
  # classification_labels: "trained_models/classification/exported/labels.txt" # Verify this path and filename

model_params:
  # Input sizes expected by the TFLite models (Height, Width)
  segmentation_input_size: [256, 256] # Verify from segmentation training config
  # Add the number of output classes for your segmentation model (e.g., 2 for binary background/foreground)
  segmentation_num_classes: 2 # Verify this number (e.g., 2 for background/foreground)
  classification_input_size: [224, 224] # Verify from classification training config

camera_intrinsics:
  # IMPORTANT: Replace with your actual camera intrinsics
  fx: 1000.0    # Focal length x (pixels) - PLACEHOLDER
  fy: 1000.0    # Focal length y (pixels) - PLACEHOLDER
  cx: 720.0     # Principal point x (pixels) - Should be near width/2
  cy: 960.0     # Principal point y (pixels) - Should be near height/2
  # Note: These values (fx, fy, cx, cy) heavily influence volume estimation accuracy.
  # Use values calibrated for the specific depth sensor/camera being used.

volume_params:
  # Optional: Define min/max depth range (in meters) for filtering point cloud
  min_depth_m: 0.1 # Adjust based on typical distance to food
  max_depth_m: 1.5 # Adjust based on typical distance/sensor range
  depth_scale_factor: 1.0 # Factor to multiply raw depth values by to get mm. Assumes raw depth is in mm.

# Optional: Add other pipeline settings if needed
# e.g., default confidence thresholds, volume estimation method flags, etc.

# --- Segmentation Model Training Data Configuration ---
segmentation_training_data:
  dataset_root_dir: "E:/_MetaFood3D_new_RGBD_videos/RGBD_videos" # Path to the segmentation dataset with masks
  image_size: [256, 256]       # Target image size (height, width) for training
  batch_size: 8                # Batch size for training (adjust based on GPU memory)
  split_ratio: 0.2             # Proportion of data for validation set (e.g., 0.2 for 20%)
  random_seed: 42              # Seed for reproducible train/validation splits
  epochs: 1                    # Number of epochs for quick testing (Reduced from 2)
  learning_rate: 0.0001        # Learning rate for the Adam optimizer
  early_stopping_patience: 5   # Patience for early stopping (reduced for quick test)
  dev_max_train_samples: 16     # Max training samples for quick dev run (Reduced from 100)
  dev_max_val_samples: 16        # Max validation samples for quick dev run (Reduced from 20)
  augmentation:
    enabled: True              # Master switch for augmentation
    horizontal_flip: True      # Enable random horizontal flips
    rotation_range: 15         # Max rotation angle in degrees (e.g., +/- 15 degrees)
    zoom_range: 0.1            # Zoom factor (e.g., 0.1 for [0.9, 1.1] zoom range, or a tuple like [-0.1, 0.1])
    width_shift_range: 0.1     # Fraction of total width for random horizontal shifts
    height_shift_range: 0.1    # Fraction of total height for random vertical shifts
    # Add other augmentation parameters here if needed (e.g., brightness, contrast)
    # brightness_range: [0.8, 1.2]
    # contrast_range: [0.8, 1.2]

# --- Classification Model Training Data Configuration (Example - adjust as needed) ---
classification_training_data:
  dataset_root_dir: "E:/_MetaFood3D_new_RGBD_videos_flipped_food/RGBD_videos_flipped_food" # Path to classification dataset
  label_map_path: "trained_models/classification/label_map.json" # Path to the label map file
  image_size: [224, 224]       # Target image size (height, width) for training
  batch_size: 16               # Batch size for training (adjusted for potential GPU memory constraints)
  split_ratio: 0.2             # Proportion of data for validation set
  random_seed: 42              # Seed for reproducible train/validation splits
  epochs: 1                    # Number of epochs for quick testing (Reduced from 2)
  learning_rate: 0.0001        # Learning rate for the optimizer
  dev_max_train_samples: 16     # Max training samples for quick dev run (Reduced from 100)
  dev_max_val_samples: 16        # Max validation samples for quick dev run (Reduced from 20)

  model_config:
    architecture: "EfficientNetV2B0"  # Example: EfficientNetV2B0, ConvNeXtTiny
    use_pretrained_weights: true
    fine_tune: false  # Set to true to unfreeze some layers of the base model
    fine_tune_layers: 10 # Number of layers from the end to unfreeze if fine_tune is true
    classification_head:
      pooling: "GlobalAveragePooling2D" # Options: GlobalAveragePooling2D, GlobalMaxPooling2D, Flatten
      dense_layers: [256] # List of units for additional dense layers before the final output layer, e.g., [512, 256]
      dropout: 0.5 # Dropout rate for the dense layers in the classification head
      output_activation: "softmax" # 'softmax' for multi-class, 'sigmoid' for multi-label

  augmentation:
    enabled: True              # Master switch for augmentation
    horizontal_flip: True      # Enable random horizontal flips
    rotation_range: 10         # Max rotation angle in degrees
    zoom_range: 0.1            # Zoom factor
    width_shift_range: 0.1     # Fraction of total width for random horizontal shifts
    height_shift_range: 0.1    # Fraction of total height for random vertical shifts
    brightness_range: [0.8, 1.2] # Applied only to images
    contrast_range: [0.8, 1.2]   # Applied only to images
  callbacks:
    model_checkpoint:
      enabled: true
      filename_template: 'model_epoch-{epoch:02d}_val_loss-{val_loss:.2f}.h5'
      monitor: 'val_loss'
      save_best_only: true
      save_weights_only: false
      mode: 'min'
    tensorboard:
      enabled: true
      histogram_freq: 1
      write_graph: true
      write_images: false
      update_freq: 'epoch'
    early_stopping:
      enabled: true
      monitor: 'val_loss'
      patience: 5
      min_delta: 0.001
      mode: 'min'
      restore_best_weights: true
    reduce_lr_on_plateau:
      enabled: true
      monitor: 'val_loss'
      factor: 0.1
      patience: 5
      min_lr: 0.000001 # 1e-6
      mode: 'min'

# --- Segmentation Model Configuration ---
