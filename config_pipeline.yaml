models:
  segmentation_tflite: "trained_models/segmentation/exported/segmentation_model.tflite" # Check if this is the correct exported model path/name
  classification_tflite: "trained_models/classification/exported/classification_model.tflite" # Check if this is the correct exported model path/name
  classification_labels: "trained_models/classification/label_map.json" # Path to the label map
  # classification_labels: "trained_models/classification/exported/labels.txt" # Verify this path and filename

model_params:
  # Input sizes expected by the TFLite models (Height, Width)
  segmentation_input_size: [256, 256] # Verify from segmentation training config
  # Add the number of output classes for your segmentation model (e.g., 2 for binary background/foreground)
  segmentation_num_classes: 2 # Verify this number (e.g., 2 for background/foreground)
  classification_input_size: [224, 224] # Verify from classification training config

camera_intrinsics:
  # IMPORTANT: Replace with your actual camera intrinsics
  fx: 150.0    # Focal length x (pixels)
  fy: 150.0    # Focal length y (pixels)
  cx: 60.0     # Principal point x (pixels) - Should be near width/2
  cy: 50.0     # Principal point y (pixels) - Should be near height/2
  # Note: These values (fx, fy, cx, cy) heavily influence volume estimation accuracy.
  # Use values calibrated for the specific depth sensor/camera being used.

volume_params:
  # Optional: Define min/max depth range (in meters) for filtering point cloud
  min_depth_m: 0.1 # Adjust based on typical distance to food
  max_depth_m: 1.5 # Adjust based on typical distance/sensor range

# Optional: Add other pipeline settings if needed
# e.g., default confidence thresholds, volume estimation method flags, etc.

# --- Segmentation Model Training Data Configuration ---
segmentation_training_data:
  dataset_root_dir: "E:/_MetaFood3D_new_RGBD_videos/RGBD_videos" # Path to the segmentation dataset with masks
  image_size: [256, 256]       # Target image size (height, width) for training
  batch_size: 8                # Batch size for training (adjust based on GPU memory)
  split_ratio: 0.2             # Proportion of data for validation set (e.g., 0.2 for 20%)
  random_seed: 42              # Seed for reproducible train/validation splits
  epochs: 2                    # Number of epochs for training (reduced for quick test)
  learning_rate: 0.0001        # Learning rate for the Adam optimizer
  early_stopping_patience: 5   # Patience for early stopping (reduced for quick test)
  dev_max_train_samples: 100     # Max training samples for quick dev run (set to null or remove for full training)
  dev_max_val_samples: 20        # Max validation samples for quick dev run (set to null or remove for full training)
  augmentation:
    enabled: True              # Master switch for augmentation
    horizontal_flip: True      # Enable random horizontal flips
    rotation_range: 15         # Max rotation angle in degrees (e.g., +/- 15 degrees)
    zoom_range: 0.1            # Zoom factor (e.g., 0.1 for [0.9, 1.1] zoom range, or a tuple like [-0.1, 0.1])
    width_shift_range: 0.1     # Fraction of total width for random horizontal shifts
    height_shift_range: 0.1    # Fraction of total height for random vertical shifts
    # Add other augmentation parameters here if needed (e.g., brightness, contrast)
    # brightness_range: [0.8, 1.2]
    # contrast_range: [0.8, 1.2]

# --- Classification Model Training Data Configuration (Example - adjust as needed) ---
classification_training_data:
  dataset_root_dir: "E:/_MetaFood3D_new_RGBD_videos_flipped_food/RGBD_videos_flipped_food" # Path to classification dataset
  label_map_path: "trained_models/classification/label_map.json" # Path to the label map file
  image_size: [224, 224]       # Target image size (height, width) for training
  batch_size: 16               # Batch size for training (adjusted for potential GPU memory constraints)
  split_ratio: 0.2             # Proportion of data for validation set
  random_seed: 42              # Seed for reproducible train/validation splits
  epochs: 2                    # Number of epochs for quick testing
  learning_rate: 0.0001        # Learning rate for the optimizer
  early_stopping_patience: 5   # Patience for early stopping
  dev_max_train_samples: 100     # Max training samples for quick dev run
  dev_max_val_samples: 20        # Max validation samples for quick dev run
  augmentation:
    enabled: True              # Master switch for augmentation
    horizontal_flip: True      # Enable random horizontal flips
    rotation_range: 10         # Max rotation angle in degrees
    zoom_range: 0.1            # Zoom factor
    width_shift_range: 0.1     # Fraction of total width for random horizontal shifts
    height_shift_range: 0.1    # Fraction of total height for random vertical shifts
    brightness_range: [0.8, 1.2] # Applied only to images
    contrast_range: [0.8, 1.2]   # Applied only to images
