data:
  image_size: [256, 256] 
  batch_size: 32
  split_ratios:
    train: 0.7
    val: 0.15
    test: 0.15
  num_classes: 2 # Number of segmentation classes (e.g., 1 for foreground + 1 for background, or N food classes + background)
  random_seed: 42 

  use_depth_map: true # Temporarily disabled for debugging
  depth_map_dir_name: "depth"

  use_point_cloud: true # Temporarily disabled for debugging
  point_cloud_root_dir: "E:/_MetaFood3D_new_Point_cloud/Point_cloud"
  point_cloud_sampling_rate_dir: "4096" 
  point_cloud_suffix: "_sampled_1.ply" 


  modalities_preprocessing:
    depth_map:
      normalization: "min_max_local" 
    point_cloud:
      num_points: 4096 
      normalization: "unit_sphere" # Options: 'unit_sphere', 'unit_cube', 'centered_only', 'none'

  # Data Augmentation (applied consistently to image and mask)
  augmentation:
    enabled: true
    horizontal_flip: true
    rotation_range: 15 
    width_shift_range: 0.1
    height_shift_range: 0.1
    zoom_range: 0.1
    apply_geometric_to_depth: true 

models:
  segmentation_tflite: "trained_models/segmentation/exported/segmentation_model.tflite"
  

# Model Configuration
model:
  architecture: "UNet" # e.g., 'UNet', 'DeepLabV3+'
  backbone: "EfficientNetB0" 
  input_shape: [256, 256, 3]
  num_classes: 2 
  activation: "sigmoid" # or "softmax" if num_classes > 2 and mutually exclusive classes
  use_pretrained_backbone_weights: true

# Optimizer Configuration 
optimizer:
  name: "Adam"
  learning_rate: 0.001

# Loss Function Configuration 
loss:
  # Common segmentation losses (can be combined)
  name: "binary_crossentropy" # e.g., 'dice_loss', 'jaccard_loss', 'binary_crossentropy', 'categorical_crossentropy'
  # Optional: Combine losses, e.g., Dice + BCE
  # combined_loss:
  #   dice_weight: 0.5
  #   bce_weight: 0.5

# Training Configuration
training:
  epochs: 100
  use_mixed_precision: true
  use_tpu: false
  metrics: ["mean_iou", "accuracy"] 

  callbacks:
    model_checkpoint:
      enabled: true
      monitor: "val_mean_iou" # Monitor MeanIoU on validation set
      mode: "max" # maximize IoU
      save_best_only: true
      save_weights_only: false
      filename_template: "unet_epoch-{epoch:02d}_val_mean_iou-{val_mean_iou:.4f}.h5"

    early_stopping:
      enabled: true
      monitor: "val_mean_iou"
      mode: "max"
      patience: 15
      restore_best_weights: true

    reduce_lr_on_plateau:
      enabled: true
      monitor: "val_mean_iou"
      mode: "max"
      factor: 0.1
      patience: 7
      min_lr: 0.000001

    tensorboard:
      enabled: true
      log_dir: "logs/segmentation/"
      histogram_freq: 0
      write_graph: false
      update_freq: "epoch"

    lr_scheduler:
      enabled: true
      name: "cosine_decay"
      alpha: 0.0

# Evaluation Configuration 
evaluation:
  # Optional: Specify model for evaluation, defaults to best checkpoint or final model
  # keras_model_filename: "unet_best_model.h5"
  batch_size: 32

# Paths Configuration 
paths:
  model_save_dir: "trained_models/segmentation/"
  log_dir: "logs/segmentation/"
  tflite_export_dir: "trained_models/segmentation/exported/"
  metadata_dir: "data/segmentation"
  metadata_filename: "metadata.json"

# Export Configuration
export:
  tflite_filename: "segmentation_model_default.tflite"

  quantization:
    type: "int8" # 'none', 'default', 'float16', 'int8'. Review for segmentation; int8 might need representative_dataset enabled.
    representative_dataset:
      enabled: true
      num_samples: 50
