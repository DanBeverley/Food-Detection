import os
import argparse
import logging
import tensorflow as tf
from tensorflow import keras
from typing import Tuple, Any
import numpy as np
from sklearn.model_selection import train_test_split
import json

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def load_data(metadata_path:str, data_dir:dir, image_size:Tuple[int, int], batch_size:int=32) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    Load and prepare classification data from metadata JSON.

    Args:
        metadata_path: Path to JSON metadata file generated by data loading scripts.
        data_dir: Root directory containing processed images (e.g., class subfolders).
        image_size: Tuple of (height, width) for image resizing.
        batch_size: Batch size for data loading.

    Returns:
        Tuple of (train_dataset, val_dataset) as tf.data.Dataset objects.

    Raises:
        FileNotFoundError: If metadata or images are missing.
        ValueError: If invalid metadata format.
    """
    try:
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
        images = [item["image_path"] for item in metadata]
        labels = [item["label"] for item in metadata]

        train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)

        def load_and_preprocess_image(path:str, label:str) -> Tuple[tf.Tensor, tf.Tensor]:
            """
            Load and preprocess a single image.
            
            Args:
                path: Path to the image file.
                label: Label string for the image.
            
            Returns:
                Tuple of (image tensor, label tensor).
            """
            image = tf.io.read_file(path)
            image = tf.image.decode_jpeg(image, channels = 3) # Adjustable
            image = tf.image.resize(image, image_size)
            image = image / 255.0 # Normalize [0, 1]
            label_ids = tf.argmax(tf.constant([label == lbl for lbl in np.unique(labels)])) # Map string label to index
            return image, label_ids
        
        # Create tf.data datasets with parallelism for efficiency
        train_dataset = tf.data.Dataset.from_tensors_slices((train_images, train_label))
        train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)
        train_dataset = train_dataset.shuffle(buffer_size = len(train_images)).batch(batch_size)

        val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))
        val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)
        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

        return train_dataset, val_dataset
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"Data loading error: {e}")
        raise

def build_model(num_classes:int, image_size:Tuple[int, int]) -> keras.Model:
    """
    Build and compile a classification model using EfficientNetV2.

    Args:
        num_classes: Number of output classes.
        image_size: Tuple of (height, width) for input shape.

    Returns:
        Compiled Keras model.

    Raises:
        ValueError: If invalid input shape.
    """
    base_model = keras.applications.EfficientNetV2B0(include_top=False, weights="imagenet", input_shape = (*image_size, 3))
    base_model.trainable = False

    model = keras.Sequential([base_model,
            keras.layers.GlobalAveragePooling2D(),
            keras.layers.Dense(256, activation="relu"),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(num_classes, activation="softmax")])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),
                 loss="sparse_categorical_crossentropy",
                 metrics=["accuracy"])
    return model

def train_model(train_dataset:tf.data.Dataset, val_dataset:tf.data.Dataset,
                epochs:int = 50, model_dir:str = "trained_models/classification") -> None:
    """
    Train the classification model with checkpoints and early stopping.

    Args:
        train_dataset: Training tf.data.Dataset.
        val_dataset: Validation tf.data.Dataset.
        epochs: Number of training epochs.
        model_dir: Directory to save model checkpoints.

    Raises:
        RuntimeError: If training fails.
    """
    try:
        num_classes = len(np.unique([label for _, label in train_dataset.unbatch().as_numpy_iterator()]))
        model = build_model(num_classes, image_size = (train_dataset.element_spec[0].shape[0], train_dataset.element_spec[0].shape[1]))
        callbacks = [
            keras.callbacks.ModelCheckpoint(os.path.join(model_dir, "model_{epoch}.h5"), save_best_only=True, monitor="val_loss"),
            keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(monitor="val_loss", patience=3, factor=0.2)
        ]
        os.makedirs(model_dir, exist_ok=True)
        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=callbacks)
        logger.info("Training completed. History saved")
    except Exception as e:
        logger.error(f"Training error: {e}")
        raise
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train classification model for food datasets.")
    parser.add_argument('--metadata_path', required=True, help="Path to JSON metadata from data loading.")
    parser.add_argument('--data_dir', required=True, help="Root directory of processed images (e.g., data/classification/processed/).")
    parser.add_argument('--epochs', type=int, default=50, help="Number of training epochs.")
    parser.add_argument('--batch_size', type=int, default=32, help="Batch size for training.")
    parser.add_argument('--image_size', nargs=2, type=int, default=[224, 224], help="Image dimensions (height width).")
    args = parser.parse_args()
    
    try:
        train_ds, val_ds = load_data(args.metadata_path, args.data_dir, tuple(args.image_size), args.batch_size)
        train_model(train_ds, val_ds, args.epochs)
    except Exception as e:
        logger.error(f"Script error: {e}")
        exit(1)