import os
import argparse
import logging
import tensorflow as tf
from tensorflow import keras
from typing import Tuple, Any
import numpy as np
from sklearn.model_selection import train_test_split
import json

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def load_data(metadata_path:str, data_dir:dir, image_size:Tuple[int, int], batch_size:int=32) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    Load and prepare classification data from metadata JSON.

    Args:
        metadata_path: Path to JSON metadata file generated by data loading scripts.
        data_dir: Root directory containing processed images (e.g., class subfolders).
        image_size: Tuple of (height, width) for image resizing.
        batch_size: Batch size for data loading.

    Returns:
        Tuple of (train_dataset, val_dataset) as tf.data.Dataset objects.

    Raises:
        FileNotFoundError: If metadata or images are missing.
        ValueError: If invalid metadata format.
    """
    try:
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
        images = [item["image_path"] for item in metadata]
        labels = [item["label"] for item in metadata]

        train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)

        def load_and_preprocess_image(path:str, label:str) -> Tuple[tf.Tensor, tf.Tensor]:
            """
            Load and preprocess a single image.
            
            Args:
                path: Path to the image file.
                label: Label string for the image.
            
            Returns:
                Tuple of (image tensor, label tensor).
            """
            image = tf.io.read_file(path)
            image = tf.image.decode_jpeg(image, channels = 3) # Adjustable
            image = tf.image.resize(image, image_size)
            image = image / 255.0 # Normalize [0, 1]
            label_ids = tf.argmax(tf.constant([label == lbl for lbl in np.unique(labels)])) # Map string label to index
            return image, label_ids
        
        # Create tf.data datasets with parallelism for efficiency
        train_dataset = tf.data.Dataset.from_tensors_slices((train_images, train_label))
        train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)
        train_dataset = train_dataset.shuffle(buffer_size = len(train_images)).batch(batch_size)

        val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))
        val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls = tf.data.AUTOTUNE)
        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

        return train_dataset, val_dataset
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"Data loading error: {e}")
        raise