data:
  metadata_path: "data/classification/metadata.json" 
  image_size: [160, 160]
  batch_size: 16 
  split_ratio: 0.2

  augmentation:
    enabled: true
    rotation_range: 20 
    width_shift_range: 0.1 
    height_shift_range: 0.1
    shear_range: 0.1
    zoom_range: 0.1
    horizontal_flip: true
    brightness_range: [0.8, 1.2] 

# Model Configuration 
model:
  architecture: "MobileNetV2" 
  use_pretrained_weights: true 
  fine_tune: false 
  fine_tune_layers: 10 

  # Classification Head Configuration
  classification_head:
    pooling: "GlobalAveragePooling2D" 
    dense_layers: [128] 
    dropout: 0.6 
    activation: "relu" 
    final_activation: "softmax"
    kernel_l2_factor: 0.001 # Add L2 regularization factor (e.g., 0.001, set to 0.0 or remove to disable)

# --- Optimizer Configuration ---
optimizer:
  name: "Adam" # 'Adam', 'SGD', 'RMSprop' (potentially 'AdamW' if tensorflow-addons installed)
  learning_rate: 0.001
  # momentum: 0.9 # Only used for SGD
  # weight_decay: 0.0001 # Only used for AdamW

# Loss Function Configuration 
loss:
  name: "sparse_categorical_crossentropy" # 'sparse_categorical_crossentropy', 'categorical_crossentropy' (requires one-hot labels)

# Metrics Configuration
metrics: ["accuracy"] # List of metrics, e.g., ['accuracy', 'sparse_top_k_categorical_accuracy']

# Training Configuration
training:
  epochs: 100 # Set epochs for full training, early stopping will manage duration
  use_mixed_precision: true
  use_tpu: false # Explicitly set to false for GPU training
  callbacks:
    model_checkpoint:
      enabled: true
      monitor: "val_loss"
      mode: "min"
      save_best_only: true
      save_weights_only: false
      filename_template: "model_epoch-{epoch:02d}_val_loss-{val_loss:.2f}.h5"
    early_stopping:
      enabled: true
      monitor: "val_loss"
      mode: "min"
      patience: 15
      restore_best_weights: true
    reduce_lr_on_plateau:
      enabled: true
      monitor: "val_loss"
      mode: "min"
      factor: 0.2
      patience: 3
      min_lr: 0.000001
    tensorboard:
      enabled: true
      log_dir: "logs/classification/" # Will be created relative to project root
      histogram_freq: 0 # Set to 1 to enable histograms (can consume disk space)
      write_graph: false
      update_freq: "epoch"
    lr_scheduler:
      enabled: true
      name: "cosine_decay"
      alpha: 0.0

# Paths Configuration (Used by Train and Export)
paths:
  # Directory where the prepare_classification_dataset.py script saves its output.
  # This directory should contain 'metadata.json' and 'label_map.json'.
  # This is also the directory from which train.py will load these files.
  data_dir: "data/classification" # General directory for processed classification data
  metadata_filename: "metadata.json" # To be found in data_dir
  label_map_dir: "data/classification" # Path relative to project root, where label_map.json is found
  label_map_filename: "label_map.json" # To be found in label_map_dir
  model_save_dir: "trained_models/classification/" 
  log_dir: "logs/classification/"
  tflite_export_dir: "trained_models/classification/exported/" 

export:
  tflite_filename: "classification_model_full_default.tflite" # Renamed for clarity

  # Quantization Settings
  quantization:
    type: "none" # Set to 'none' for initial Keras training. Change to 'int8' or 'float16' for export later.
    representative_dataset: 
      enabled: true 
      num_samples: 100 